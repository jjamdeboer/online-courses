IN THE ORIGINAL MAPREDUCE THERE IS ONLY ONE JOBTRACKER, WHICH MAKES IT A VULNERABLE COMPONENT IN THE SYSTEM
ALSO, SINCE THERE IS ONLY ONE OF THESE COMPONENT: IT DOES NOT SCALE WELL

TO LIST THE MOST SERIOUS LIMITATIONS OF THE ORIGINAL MAPREDUCE:
- SCALABILITY (ONE JOBTRACKER, SO NO MULTIPLE TASKS ALLOWED)
- RESOURCE UTILIZATION (IDEM, SINCE MOST OF THE CLUSTER IS NOT MAPPING)
- VERY INTOLERANT TO ANY OTHER SCHEME THAN MAPREDUCE, THIS SOLVED PARTIALLY BY YARN

JOBTRACKER IN MAPREDUCE HAD MULTIPLE MAIN TASKS: MAINTAINING A LIST OF THE LIVE NODES; MAINTAINING A LIST OF AVAILABLE MAP AND REDUCE SLOTS; ALLOCATING THE REMAINING JOBS TO AVAILABLE SLOTS; COORDINATION OF ALL TASKS, WHICH INVOLVED COMMUNICATING WITH TASKTRACKERS (STARTING, MONITORING, RESCHEDULING FAILURES)
THE TASKTRACKERS IN THIS MODEL ONLY HAVE A HANDFUL OF MAP AND REDUCE FUNCTIONS TO KEEP TRACK OF, WHEREAS THE JOBTRACKER HAS AN EXTENSIVE LIST OF TASKS

IN YARN, THE JOBTRACKER IS SPLIT INTO TWO INSTANCES: RESOURCE MANAGMENT INTO ONE INSTANCE AND JOB SCHEDULING/MONITORING IN ANOTHER INSTANCE:
- GLOBAL RESOURCE MANAGER (RM) TOGETHER WITH THE NODE MANAGER (NM): HAS OVERVIEW OF AND DECIDES WHICH TASKS GO TO WHICH NODES
    - SCHEDULER: ALLOCATING RESOURCES FOR VARIOUS RUNNING APPLICATIONS (SIMPLE SCHEDULER, DOES NOT RESTART WHEN FAILURE TOOK PLACE, NOR MONITORS OR TRACKS APPLICATIONS); WORKS WITH RESOURCE CONTAINERS; ORIGINALLY ONLY SEQUENTIALLY, BUT NOW COMPATIBLE WITH OTHER FORMS OF SCHEDULING
    - APPLICATIONS MANAGER: ACCEPTS JOBS, NEGOTIATES THE FIRST CONTAINER FOR THE JOB AND IS RESPONSIBLE FOR RESTARTING AM IN CASE OF FAILURE
[- NODE MANAGER: PER MACHINE MANAGER FOR MONITORING CONTAINERS (MEMORY, CPU) AND REPORTING TO RM/SCHEDULER; NUMBER OF MAP AND REDUCE SLOTS IS VARIABLE DEPENDING ON CPU OR SPEED, WHEREAS WITH TASK MANAGERS THIS AMOUNT WAS FIXED]
- APPLICATIONMASTER (AM): APPLICATIONS ARE EITHER A MAPREDUCE JOB OR A DIRECTED ACYCLIC GRAPH (DAG) OF JOBS, ONE MANAGER PER JOB OR DAG; NEGOTIATE WITH RM FOR RESOURCES/CONTAINERS AND COOPERATES WITH NM TO EXECUTE AND MONITOR THE TASKS

SINCE AM AND RM ARE NOW SPLIT, YARN (BASICALLY THE SYSTEM SETUP) IS VERY TOLERANT FOR OTHER TASK FORMS THAN MAPREDUCE (SUCH AS SPARK)

FURTHERMORE, YARN ALLOWS FOR:
- SCALABILITY: SINCE JOBTRACKER IS SPLIT
- MULTI-TENANCY: MULTIPLE USERS CAN USE THE SAME CLUSTER MORE EASILY, SINCE SCALED IT DOES NOT SIGNIFICANTLY SLOW DOWN
- COMPATIBILITY: BACKWARDS COMPATIBILITY WITH MAPREDUCE
- SERVICEABILITY
- HIGH-CLUSTER UTILIZATION: SINCE SCALES BETTER AND SINCE NODE MANAGERS ARE DYNAMIC (THE NUMBER OF CONTAINERS THEREIN), THE RESOURCES ARE MORE EFFICIENTLY USED
- RELIABILITY/AVAILABILITY: WORK IS DONE ON THE SINGLE NAMENODE THAT EXISTS, BUT IMPROVEMENTS ON PREVIOUS VERSIONS HAVE BEEN MADE

mapreduce hits limits at 4000 nodes and 40000 tasks; yarn 10.000 nodes and 100.000 tasks
