MAPREDUCE ORIGINALLY DESIGNED BY GOOGLE
NODE IS A COMPUTER, RACK IS A COLLECTION OF 30/40 COMPUTERS IN CLOSE VICINITY

MAPREDUCE HAS FOLLOWING PHILOSOPHY:
- SPREAD DATA OVER LARGE CLUSTER
- BRING PROGRAMS TO DATA, NOT DATA TO PROGRAMS; THIS REQUIRES PROGRAMS WRITTEN IS A SPECIAL WAY
THEREFORE, THE CLUSTER BOTH READS AND PROCESSES THE DATA
SINCE MAPREDUCE IS BASED ON FUNCTIONAL PROGRAMMING, NO INTERMEDIATE STATES/OBJECTS ARE USED WITHIN PROGRAMS, SIMPLY FUNCTIONS ARE USED

THE HADOOP DISTRIBUTED FILE SYSTEM (HDFS) IS AT THE HEART OF MAPREDUCE; IT SPREADS THE DATA/BLOCKS OVER THE CLUSTER (AND REPLICATES THE DATA DUE TO FEAR FOR FAILURE OF NODES)
THE DEGREE OF REPLICATION IS DECIDED BY THE ADMINISTRATOR, BUT THE DEFAULT IS THREE REPLICATIONS, WITH ONE ON ONE RACK AND TWO ON ANOTHER BUT IDENTICAL RACK
HDFS IS WHERE THE DATA IS STORED AND LINKS TOGETHER THIS DATA

THE FILE INPUT IS SPLIT IN MANY SMALL PIECES AND A FUNCTION IS EXECUTED ON THEM (MAP); THE RESULT IS THEN GATHERED AND MERGED TOGETHER FOR ONE RESULT (REDUCE)
IF SPECULATIVE EXECUTION IS ENABLED, THE SAME MAP IS EXECUTED ON MULTIPLE NODES, SO THAT A SLOW NODE DOES NOT SLOW THE WHOLE PROCESS DOWN

FOR MAPREDUCE JOBS, THE PHILOSOPHY IS FIRST-IN-FIRST-OUT (FIFO)/SEQUENTIALLY
THE JOBTRACKER (MASTER NODE) BREAKS JOBS INTO MAP AND REDUCE STEPS; FINDS WHERE THE BLOCKS ARE LOCATED AND THEN BRINGS THE RELEVANT MAP PROGRAMS TO THERE; FOR REDUCE, THE MAPPED DATA IS SENT TO THE RELEVANT REDUCER NODES, WHICH PERFORM THE REDUCE STEP

API'S (APPLICATION PROGRAMMIN INTERFACES) THAT ARE USED (WHICH ARE INDEPENDENT OF EXECUTION ENVIRONMENT): org.apache.mapred (OLD), org.apache.mapreduce (NEW)

ACTUALLY MAPREDUCE CONSISTS OF FOUR PHASES: MAP, COMBINE, SHUFFLE, REDUCE:
- MAP: FUNCTIONS THAT ARE LOCATED ON THE DATANODES OF THE RELEVANT DATA AND ARE EXECUTED; THE RESULT IS STORED LOCALLY TEMPORARILLY; PRODUCES KEY-VALUE-PAIRS
- COMBINE (OPTIONAL): PER NODE, ALL THE PRODUCED KEY-VALUE-PAIRS ARE FIRST GROUPED TOGETHER BEFORE SHUFFLING; THIS IS ONLY POSSIBLE IF THIS DATA IS LOCALLY ADDITIVE AND NO INFORMATION IS LOST WHEN ALL THIS DATA IS COMBINED TO ONE KEY-VALUE-PAIR; RUNS THE REDUCE TASK, BUT THEN LOCALLY
- SHUFFLE: THE OUTPUT OF THE MAP TASKS ARE FIRST SHUFFLED/IDENTICAL KEYS GROUPED TOGETHER ON RELEVANT WORKER NODES
- REDUCE: APPLYING THE REDUCE FUNCTION TO THE IDENTICAL KEYS ON THE SAME WORKER NODE; THE OUTPUT IS THEN SENT AND DUPLICATED TO SEVERAL NODES ON THE HDFS; ALSO REDUCE PRODUCES KEY-VALUE-PAIRS

SINCE BIG FILES REQUIRE MORE BLOCKS, MORE NODES ARE INVOLVED IN THE MAPREDUCE TASKS FOR THE FILE, SO THE MAP-STEP SCALES ACCORDINGLY

THE WORDCOUNT PROGRAM IS THE 'HELLO WORLD!' OF HADOOP

THERE ARE TEN MAJOR STEPS IN EVERY MAPREDUCE:
- JOB CLIENT IS NOTIFIED THERE IS A MAPREDUCE JOB
- THIS IS SENT TO THE JOBTRACKER, WHICH CREATES JOB ID
- THE JOB CLIENT SENDS THE MAP AND REDUCE FUNCTIONS TO THE HDFS
- THE JOB CLIENT NOTIFIES THE JOBTRACKER THAT THE JOB CAN START
- THE JOBTRACKER CALCULATES THE DATA SPLIT FOR OPTIMAL MAPPING PROCESSING THAT MAXIMIZES THE SPEED
- THESE INPUT SPLITS ARE LOCATED IN THE HDFS AND THE LOCATIONS SENT TO THE JOBTRACKER
- TASKTRACKER IS SENDING HEARTBEAT MESSAGES TO THE JOBTRACKER
- TASKTRACKER OBTAIN THE RELEVANT CODE FROM THE HDFS
- JAVA VIRTUAL MACHINE IS LAUNCHED BY EACH TASKTRACKER (LOCALLY) 
- THAT RUNS THE RELEVANT MAP OR REDUCE
MAPS ARE STORED LOCALLY (THUS NOT IN HDFS), REDUCE ARE STORED IN HDFS AND THUS IS DUPLICATED

CLASSES INSIDE HADOOP CODE:
- INPUTSPLITTER: SPLITS A FILE
- RECORDSPLITTER: READS A SPLIT AS A SEQUENCE OF RECORDS
- INPUTFORMAT: TAKES RECORD AND TRANSFORMS IT IN KEY-VALUE-PAIRS (WHICH IS THEN GIVEN TO THE MAP-TASK)

FILES ARE STORED AS SPLITS OR BLOCKS WITH STANDARD SIZES 64-128MB

TASKTRACKERS COMMUNICATE TO JOBTRACKER; IF THIS DOES NOT HAPPEN, THE JOBTRACKER ASSUMES THAT THE CORRESPONDING NODE HAS CRASHED AND WILL RE-EXECUTE ALL THE MAPPING OR ALL/PART OF THE REDUCING (DEPENDING WHAT ALREADY IS FINISHED) AT OTHER NODES
FAILURES CAN HAPPEN AT:
- TASK LEVEL
- TASKTRACKER LEVEL
- JOBTRACKER LEVEL
