# SUPERVISED LEARNING:
# LABELED DATA, MAKE A PREDICTION OF THE BEHAVIOUR/VALUE OF ONE OR MULTIPLE LABELS
#  OFTEN DATA IS SPLIT IN THREE: TRAINING, VALIDATING AND THEN FINALLY TESTING
#  THIS IS DESIREABLE, SINCE OTHERWISE GROSS OVERFITTING CAN OCCUR
#  CLASSIFICATION METRICS ARE: 
    #  ACCURACY: CORRECT/ALL -- THE DIAGONAL OF THE CONFUSION MATRIX/THE WHOLE MATRIX; GOOD FOR BALANCED EQUAL-VALUE CLASSES/DATA
    #  MISCLASSIFICATION: MISCLASSIFIED ALL -- THE OFF-DIAGONAL OF THE CONFUSION MATRIX/THE WHOLE MATRIX; IDEM GOOD FOR..., ETC
    #  RECALL: TP / TP + FN -- RIGHT POSITIVES / RIGHT POSITIVES + WRONG NEGATIVES (SHOULD HAVE BEEN POSITVES)
    #  PRECISION: TP / TP + FP -- RIGHT POSITIVES / RIGHT POSITIVES + WRONG POSITIVES (SHOULD HAVE BEEN NEGATIVES)
    #  F1-SCORE: HARMONIC MEAN OF PRECISION OF RECALL == 2 * P * R / P + R
    #  CONFUSION MATRIX: X == TRUE, Y == PREDICTED
#  REGRESSION/FITTING METRICS ARE: L0, L1, L2
    #  L0: NUMBER OF ERRORS
    #  L1: MEAN ABSOLUTE ERROR
    #  L2: MEAN SQUARED ERROR; HAS DOWNSIDE OF BEING UNFAIR TO DIFFERENT SCALES, SO OFTEN NORMALIZATION NEEDS TO BE DONE
#  STRUCTURE IN PYTHON FOR SUPERVISED LEARNING IS: 
    #  from sklearn.[family] import [model]; E.G.: from sklearn.linear_model import LinearRegression
    #  from sklearn.model_selection import train_test_split; X_trn, X_tst, y_trn, y_tst = train_test_split( X, y, test_size = 0.3 )
    #  model.fit( X_trn, y_trn )
    #  predictions = model.predict( X_tst )
    #  evaluation == SOMETHING WITH predictions VS y_tst

# UNSUPERVISED LEARNING:
# UNLABELED DATA, TRY TO GROUP THEM TOGETHER AND/OR LABEL IT

# REINFORCEMENT LEARNING:
# TRY TO PERFORM AN ACTION BASED ON 'EXPERIENCE'. THROUGH TRIAL AND ERROR WHICH TACTICS OFFERS GREATEST BENEFITS. 
# THREE COMPONENTS: AGENT (THE ONE THAT TAKES ACTION), ENVIRONMENT (SURROUNDINGS) AND ACTIONS (SUSTAINED ACTIONS).

#  BIAS-VARIANCE TRADEOFF: A MORE BIASED MODEL MIGHT HAVE STRONGER FIT, BUT ALSO A HIGHER VARIANCE/JITTER
#  REMEMBER THAT FLEXIBILITY COMES AT THE COST OF OVERFITTING
#  LOW BIAS IS THAT (AVERAGE) PREDICTIONS ARE ABOUT RIGHT, LOW VARIANCE IS THAT SPREAD OF DATA IS LOW
