ARE USED FOR:
- SIGNAL AND IMAGE RECOGNITION
- TEXT/CHARACTER RECOGNITION
- OBJECT RECOGNITION
- FACE DETECTION
- RECOMMENDER SYSTEMS
- NATURAL LANGUAGE PROCESSING
- SPEECH RECOGNITION
IT WAS DEVELOPED TO OBTAIN GOOD REPRESENTATIONS OF IMAGES TO SUPPORT RECOGNITION:
- OBJECT RECOGNITION
- ROBUSTNESS AGAINST TRANSLATION, ROTATION, ILLUMINATION, ETC.
HIERARCHICAL FEATURE EXTRACTION: FIRST LAYERS ARE FOR SIMPLE FEATURES -> COMBINATION OF THESE SIMPLE FEATURES -> ETC.
NEEDS MANY TRAINING EXAMPLES

FEATURE EXTRACTION FOR IMAGE RECOGNITION IS MOST IMPORTANT FOR A NETWORK TO HAVE; CONVOLUTIONAL NEURAL NETWORKS DOES THIS AUTOMATICALLY
CAN BE USED WELL FOR (IMAGE) CLASSIFICATION
MNIST: NORMALIZED AND CENTERED (WITH FIXED SIZE) DATASET OF HANDWRITTEN DIGITS
PRE-PROCESSING DATA -> TRAINING NETWORK -> INFERENCE

STRUCTURE: CONVOLUTIONAL LAYERS (CONVOLUTES THE INPUT USING A FILTER/KERNEL; OUTPUT OF CONVOLUTION PROCESS IS FEATURE MAP; KERNELS ARE UPDATED DURING TRAINING, INITIALIZED RANDOMLY; EXTRACTS (BASIC) FEATURES), POOLING LAYERS (POOLS THE CONVOLUTED INPUT), FULLY CONNECTED LAYERS (CONNECTS EVERY NEURON IN PREVIOUS LAYER TO EVERY NEURON IN NEXT LAYER)
CONVOLUTION + KERNEL -> RECTIFIED LINEAR UNIT (TRANSFORMING EACH NEGATIVE VALUE TO 0) -> POOLING (FOR EXAMPLE MAX POOLING) TO REDUCE THE DIMENSION, STRIDE OF THIS POOLING PROCESS IS A HYPERPARAMETER -> FULLY CONNECTED LAYER WHICH FIRST UNWRAPS PREVIOUS MATRICES INTO VECTORS AND THEN REDUCES DIMENSION STILL, CONVERTING ALL THESE VECTORS INTO A SINGLE VECTOR OF LOW DIMENSION -> SOFTMAX OR OTHER ACTIVATION FUNCTION TO CLASSIFY THE INPUT
IT CAN FIRST REPEAT CONVOLUTION + KERNEL -> RECTIFIED LINEAR UNIT -> POOLING, FOR SEVERAL TIMES
THEN IT CAN REPEAT FULLY CONNECTED LAYER -> RECTIFIED LINEAR UNIT, FOR SEVERAL TIMES, THEN PUTTING IT INTO SOFTMAX
FOR EACH HIDDEN LAYER IT IS CUSTOM TO USE A LOT OF CONVOLUTION NODES/KERNELS
WHAT IS LEARNED IS THE KERNELS/WEIGHT + BIASES
