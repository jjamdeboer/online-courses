SEQUENTIAL DATA IS WHEN DATA IS DEPENDENT ON PREVIOUS DATA POINTS
EXAMPLES: STOCK, WEATHER, GENES, SENTENCES
FOR TRADITIONAL NEURAL NETWORKS, DATA IS ASSUMED TO BE INDEPENDENT
THE STATE OF THE RECURRENT NETWORK IS FED INTO THE NETWORK ITSELF AT EACH TIME STEP, USING ALL PREVIOUS POINTS AS INFORMATION, BEING ABLE TO DETECT CORRELATIONS
THREE WEIGHT MATRICES: ONE FOR THE PREVIOUS STATE AND ONE FOR THE DATA  TO CALCULATE THE NEXT STATE AND ONE FOR THE NEW STATE TO CALCULATE THE OUTPUT
INPUT-OUTPUT FOR RECURRENT NEURAL NETWORKS CAN BE EVERYTHING: ONE-TO-MANY (IMAGE TO OBJECTS RECOGNIZED, ALTHOUGH IMAGE IS MANY CORRELATED PIXELS, SO NOT REALLY FAIR), MANY-TO-MANY (SOUND TO SENTENCE), MANY-TO-ONE (STOCK TO PRICE FOR TOMORROW/SENTIMENT ANALYSIS)
DOWNSIDE: MUST REMEMBER/INCORPORATE ALL PREVIOUS STATES, WHICH IS EXPENSIVE; VERY SENSITIVE TO PARAMETER ADAPTION, SO GRADIENT DESCENT METHODS MIGHT NOT WORK, SUCH AS HAVING A VANISHING/EXPLODING GRADIENT PROBLEM
THE LONG SHORT-TERM MEMORY (LSTM) MODEL: THREE GATES, THE WRITE GATE (WRITING DATA INTO THE MEMORY CELL), THE READ GATE (READING DATA FROM MEMORY CELL), THE FORGET GATE (DELETES PART OF THE MEMORY CELL); TWO INPUTS WITH RESPECT TO THE DATA, THE TOTAL OUTPUT OF PREVIOUS STEP AND THE HIDDEN STATE/MEMORY
THE ORIGINAL RECURRENT NETWORK ONLY HAS ONE INPUT WITH RESPECT TO THE DATA: ITS STATE/MEMORY
STACKING LONG SHORT-TERM MEMORY NETWORKS IS ALSO POSSIBLE, WHERE FOR THE SECOND NETWORK, THE OUTPUT OF THE FIRST NETWORK IS CONSIDERED THE DATA
FOR LANGUAGE PROCESSING, WORDS ARE CONVERTED INTO VECTORS, MOSTLY USING WORD EMBEDDINGS: N-DIMENSIONAL VECTOR TO UNIQUELY DEFINE EACH WORD, MOSTLY A 200-DIMENSIONAL VECTOR; THESE VECTORS ARE UPDATED EVERY TIME THE WORD IS ENCOUNTERED
