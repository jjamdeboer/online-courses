#DATA ACQUISITION IS PROCESS OF LOADING/COLLECTING DATA
#DATAFRAME.dtypes RETURNS ALL DATA TYPES OF THE COLUMNS OF A DATAFRAME
#DATAFRAME.describe() ARE QUICK STATISTICS ON THE DATAFRAME; TO INCLUDE OBJECT/STRING-TYPES, USE .describe(include = 'all')
#DATAFRAME.info() = DATAFRAME.head(30) + DATAFRAME.tail(30)

#DATA PRE-PROCESSING = DATA CLEANING = DATA WRANGLING
#TO DROP ROWS OR COLUMNS WITH NAN USE DATAFRAME.dropna(axis=0/1 (DROP DROW/COLUMN; STANDARD IS 0), inplace = True/False, subset = ['COLUMN NAME', 'COLUMN NAME', ...] (FOR DROPPING ONLY THE ROWS/COLUMNS BASED ON THIS SUBSET, BUT LEAVE THE REST INTACT))
#AFTER DROPPING, ONE COULD RESET THE INDEX USING DATAFRAME.reset_index(drop=True (DROPPING OLD INDEX), inplace=True/False)
#FOR REPLACING VALUES, USE DATAFRAME.replace(OLD VALUE, NEW (CALCULATED) VALUE); EXAMPLE: DATAFRAME['COLUMN NAME'].replace(numpy.nan, DATAFRAME['COLUMN NAME'].mean())
#FOR RENAMING COLUMNS/ROWS, USE DATAFRAME.rename(columns = ['OLD COLUMN NAME': 'NEW COLUMN NAME'], inplace = True/False)
#CONVERTING DATA TYPES, USE DATAFRAME[[COLUMN SUBSET]].astype('int'/'float'/...)
#SIMPLE FEATURE SCALING DIVIDES EVERY FEATURE BY THE MAXIMUM VALUE OF THAT FEATURE; UNFIT FOR DATA THAT IS NOT FIXED YET OR SETS THAT MAY BE EXTENDED; MIN-MAX IS (OLD VALUE - MINIMUM VALUE)/(MAXIMUM VALUE - MINIMUM VALUE); Z-SCORE IS (OLD VALUE - MEAN VALUE)/STANDARD DEVIATION
#IDENTIFYING NULL-VALUES CAN BE DONE WITH DATAFRAME.isnull(), RETURNING A DATAFRAME WITH BOOLEANS

#BINNING IS PROCESS OF GROUPING FEATURES TOGETHER, E.G.: 0-50, 50-100, ...
#CONVERTING CATEGORICAL COLUMNS TO NUMERICAL VALUES CAN BE DONE, AMONGST OTHERS, WITH pandas.get_dummies(DATAFRAME['COLUMN NAME']), WHICH CREATES A DATAFRAME WITH THE CATEGORIES AS COLUMNS AND THEN 0/1 WHETHER THIS ROW HAS THAT CATEGORY
#TO SEE OCCURANCE OF EACH CATEGORY, USE DATAFRAME['COLUMN NAME'].value_counts()

#EXPLORATORY DATA ANALYSIS (EDA)
#GROUPBY IS EXTREMELE CONVENIENT TO INVESTIGATE SUB-FEATURES: DATAFRAME.groupby(['COLUMN NAME', 'COLUMN NAME', ...], as_index = True/False).GROUPING FUNCTION SUCH AS mean(), max(), std()
#GROUPBY CAN BE COMBINED (BUT THIS CAN ALSO BE USED REGARDLESSLY) WITH A PIVOT TABLE: DATAFRAME.pivot(index = ['COLUMN NAME THAT IS DESIREABLE AS INDEX', ...], columns = ['COLUMN NAME THAT IS DESIREABLE AS A COLUMN', ...])
#TO GET THE CORRELATIONS BETWEEN THE COLUMNS, USE DATAFRAME[['COLUMN NAME', 'COLUMN NAME', ...]].corr()
#FOR UNIQUE VALUES OF A COLUMN, USE DATAFRAME[['COLUMN NAME', 'COLUMN NAME', ...]].unique() OR DATAFRAME[['COLUMN NAME', 'COLUMN NAME', ...]].nunique()
#FOR RESIDUAL PLOTS, USE seaborn.residplot(DATAFRAME[['COLUMN NAME', 'COLUMN NAME', ...]], DATAFRAME['COLUMN NAME'])
#DISTRIBUTION PLOTS CAN BE USED TO SEE HOW FAR OFF THE MODEL IS, USE seaborn.distplot
#FOR POLYNOMiAL REGRESSION, USE: numpy.polyfit(x, y, ORDER) AND numpy.polyld(numpy.polyfit(x, y, ORDER))

#CONSIDER USING A TRAIN-TEST-SPLIT FOR EVALUATING THE PERFORMANCE OF A MODEL
#GRID SEARCH IS A MEANS OF SCANNING OVER THE HYPERPARAMETERS WITH CROSS-VALIDATION, DON'T USE IT
#RIDGE REGRESSION SPLITS DATA AND USES EVERY PERMUTATION FOR TRAIN-TEST SPLITS
