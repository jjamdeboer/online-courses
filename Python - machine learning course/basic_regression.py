#IN REGRESSION THERE IS ONE 'DEPENDENT' VARIABLE (STATE, TARGET) AND POSSIBLY MULTIPLE 'INDEPENDENT' VARIABLES (ATTRIBUTES)
#THE ATTRIBUTES/FEATURES CAN BE EITHER CONTINUOUS OR CATEGORICAL
#INTERCEPT AND GRADIENTS ARE THE COEFFICIENTS OF THE LINEAR EQUATION

#LINEAR REGRESSION IS FAST, HAS NO TUNABLE PARAMETERS (LIKE K-NEAREST-NEIGHBOURS), AND IS EASY TO UNDERSTAND AND VERY INTERPRETABLE
#MULTIPLE/MULTIDIMENSIONAL LINEAR REGRESSION CAN ALSO BE EXPRESSED AS PARAMETER/WEIGHT VECTOR DOT FEATURE VECTOR: theta^T * X 
#FOR ESTIMATING theta, ONE CAN SOLVE DIRECTLY BY MATRIX MULTIPLICATION, OR THROUGH AN OPTIMIZATION PROCESS SUCH AS GRADIENT DESCENT, WHICH IS DESIRABLE FOR HUGE DATASETS

#FOR TESTING MODELS, ONE CAN TRAIN AND TEST ON THE SAME MODEL OR USE A TRAIN-TEST-SPLIT
#TRAINING ACCURACY AND OUT-OF-SAMPLE ACCURACY; BOTH ARE PROBLEMS IN TRAINING AND TESTING ON THE SAME DATA AND USING TRAIN-TEST-SPLITS; AN ARTIFICIAL SOLUTION IS K-FOLD CROSS-VALIDATION
#SOME EVALUATION METRICS ARE MEAN ABSOLUTE ERROR, MEAN SQUARED ERROR, ROOT MEAN SQUARED ERROR, RELATIVE ABSOLUTE ERROR (ABSOLUTE ERROR/ABSOLUTE ERROR OF THE MEAN VALUE OF THE PREDICTOR), RELATIVE SQUARED ERROR (IDEM, WITH MEAN SQUARED ERROR), R^2 (1 - RELATIVE SQUARED ERROR)

#IMPORTANT COMMANDS (SIGMOID IS DEFINED BEFORE IN THE CODE, ANOTHER PREDEFINED FUNCTION CAN BE INSERTED THERE):
#from scipy.optimize import curve_fit
#popt, pcov = curve_fit(sigmoid, xdata, ydata)
#print the final parameters
#print(" beta_1 = %f, beta_2 = %f" % (popt[0], popt[1]))
