#IN CLASSIFICATION THE TARGET IS CATEGORICAL
#TYPES ARE: DECISION TREES, NAIVE BAYES, LINEAR DISCRIMINANT ANALYSIS, K-NEAREST NEIGHBOUR, LOGISTIC REGRESSION, NEURAL NETWORKS, SUPPORT VECTOR MACHINES

#K-NEAREST NEIGHBOURS: SELECT K, CALCULATE DISTANCE OF UNKNOWN CASE FROM THE LABELED DATA, SELECT THE K NEAREST NEIGHBOURS AND DO MAJORITY VOTE TO SELECT CATEGORY OF THE UNKNOWN CASE

#THERE ARE THREE METRICS FOR CLASSIFICATION THAT ARE IMPORTANT: JACCARD INDEX (INTERSECTION PREDICTED AND TRUE VALUES / ALL PREDICTED AND TRUE VALUES), F1-SCORE (PRECISION * RECALL * 2 / (PRECISION + RECALL)), PRECISION (TRUE POSITIVES / TRUE POSITIVES + FALSE POSITIVES), RECALL (TRUE POSITIVES / TRUE POSITIVES + FALSE NEGATIVES) AND LOG LOSS (-1/N SUM y LOG (ŷ) (1 - y) LOG (1 - ŷ)); FOR VALUES THAT ARE EXPRESSED IN PROBABILITIES SUCH AS IS DONE IN LOGISTIC REGRESSION)

#FOR DECISION TREES: EACH NODE CORRESPONDS TO A TEST, EACH BRANCH THE OUTCOME OF TESTS AND EACH LEAF A CATEGORY
#FOR DECISION TREES THE NEXT SPLIT IS DETERMINED USING ENTROPY AND ENTROPY DECREASE

#LOGISTIC REGRESSION IS ALMOST EXCLUSIVELY USED FOR BINARY TARGETS AND WHEN PROBABILISTIC CASTING IS DESIRABLE AND THERE IS A LINEAR DECISION BOUNDARY (LINEAR IN THE COEFFICIENTS, AS ALWAYS); ALL INDEPENDENT VARIABLES SHOULD BE CAST INTO NUMERIC VALUES, INCLUDING THE CATEGORICAL ONES
#FOR ALL REGRESSION MODELS, SCALING IS ESSENTIAL
#COST FUNCTION GIVES DIFFERENCE BETWEEN PREDICTED AND ACTUAL VALUES, SO RELATED TO ERROR FUNCTION

#WITH SUPPORT VECTOR MACHINES, THE FEATURES ARE MAPPED UNTO A HIGH DIMENSIONAL FEATURE SPACE (INCLUDING, FOR EXAMPLE, SOME FEATURES SQUARED) AND A SEPARATOR/HYPERPLANE IS APPLIED TO THAT SPACE
#MAPPING UNTO A HIGHER DIMENSIONAL SPACE IS KERNELLING, THE FUNCTION WITH WHICH TO MAP THE KERNEL
#FOR DETERMINING THE BEST SEPARATORS/HYPERPLANES, ONE CAN THINK OF USING THE MARGIN OF THE HYPERPLANE (DISTANCE TO NEAREST INSTANCE) AS ONE METRIC; INSTANCES CLOSEST TO HYPERPLANE ARE SUPPORT VECTORS
#SUPPORT VECTOR MACHINES ARE ACCURATE AND MEMORY-EFFICIENT (SINCE IT ONLY USES THE SUPPORT VECTORS TO ACHIEVE SOMETHING); THE DOWNSIDE IS THAT OVERFITTING OCCURS EASILY AND NO PROBABILITY FOR THE CATEGORIES IS PROVIDED, RATHER A BINARY VALUE, AND ARE VERY COST-INTENSIVE
#SUPPORT VECTOR MACHINES ARE EFFECTIVE IN IMAGE RECOGNITION, TEXT ANALYSIS AND GENE CLASSIFICATION
