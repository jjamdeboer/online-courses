RESTRICTED BOLTZMANN MACHINES: USED FOR PATTERN RECOGNITION AND RECONSTRUCTS THE INPUT; TWO LAYERS (VISIBLE AND HIDDEN LAYER) CONNECTED, RESTRICTED BECAUSE THE NODES WITHIN A LAYER ARE NOT CONNECTED; TWO-WAY TRANSLATOR ENCODES THE INPUTS AND THEN PROPAGATES THIS BACK TO DECODES THAT INPUT/OUTPUT; HIGH ACCURACY FOR SIMPLE PATTERNS WHEN WELL-TRAINED; THE DISTANCE MEASURE TO COMPARE THE DECIPHERED THROUGHPUT TO THE INPUT IS THE KL DIVERGENCE; CAN BE USED BOTH FOR SUPERVISED AND UNSUPERVISED LEARNING; A RESTRICTED BOLTZMANN MACHINE IS A FEATURE EXTRACTOR/AUTOENCODER
HINTON, BENGIO

DEEP BELIEF NETWORKS: INSTEAD OF VIEWING DEEP BELIEF NETWORKS AS SUCH AND NEEDING TO TRAIN THE NETWORK WITH BACKPROPAGATION BACK TO FRONT, DEEP BELIEF NETWORKS ARE SEEN AS STACKED RESTRICTED BOLTZMANN MACHINES/AUTOENCODERS (SO THAT OUTPUT OF ONE LAYER ARE USED AS INPUT FOR OTHER LAYER); TO FINISH TRAINING OF A DEEP BELIEF NETWORK, IT IS DESIRABLE TO HAVE SOME LABELED DATA SO THAT THE DISTINGUISHED PATTERNS CAN BE GIVEN NAMES AND RECOGNIZED BY HUMANS (MAYBE ALSO USING BACKPROPAGATION THEN TO INCREASE THE ACCURACY); SUPERVISED LEARNING, HOWEVER, POTENTIAL UNSUPERVISED LEARNING POSSIBLE; GOOGLE
HINTON

CONVOLUTIONAL NETWORKS: USED FOR COMPUTER VISION; THE INPUT LAYER IS CONNECTED TO A CONVOLUTION LAYER, WHICH PERFORMS CONVOLUTIONS, AND WHICH HAS THE SAME WEIGHTS AND BIASES MODULO A NUMBER; THE INPUT LAYER SEEMS TO SPATIALLY SEPARATE DIFFERENT PARTS OF THE INPUT (MAKING A GRID ON AN IMAGE FOR EXAMPLE); AFTER CONVOLUTION, RECTIFIED LINEAR UNITS AND POOLING TAKES PLACE; THIS NETWORK IS TRAINED USING BACKPROPAGATION, SO VANISHING GRADIENT MIGHT BE A PROBLEM, WHICH IS SLIGHTLY LESS FOR RECTIFIED LINEAR UNITS; POOLING BRINGS TOGETHER MULTIPLE FEATURES, AS TO EXTRACT THE MOST IMPORTANT ONES; CONVOLUTIONAL LAYERS, RECTIFIED UNIT LAYERS AND POOLING LAYERS ARE OFTEN REPEATED IN BLOCKS MULTIPLE TIMES; CAN ONLY BE USED FOR SUPERVISED LEARNING AND CONSUMES A HUGE DATASET; FACEBOOK
LECUN

RECURRENT NETWORKS: PATTERNS CHANGING OVER TIME; OUTPUT OF THE LAYER IS ADDED TO THE NEW INPUT FOR THAT LAYER; DOCUMENT CLASSIFICATION, IMAGE CAPTIONING, DEMAND IN SUPPLY CHAIN FORECASTING, VIDEO CLASSIFICATION; STACKING RECURRENT NETWORKS IMPROVES PERFORMANCE; USES BACKPROPAGATION AND SUFFERS FROM THE VANISHING GRADIENT; TO ADDRESS THE VANISHING GRADIENT, SEVERAL TECHNIQUES OF WHICH GATING (SELECTION OF HISTORY; GRU, LSTM) IS MOST IMPORTANT; FEEDFORWARD NETWORKS MOST IMPORTANT FOR CLASSIFICATION, RECURRENT NETWORKS MOST IMPORTANT FOR PREDICTING SEQUENCES 
SCHMIDHUBER, HOCHREITER, GRAVES

AUTOENCODERS: RESTRICTED BOLTZMANN MACHINES ARE EXAMPLES, DENOISING AND CONTRACTIVE AUTOENCODERS OTHER EXAMPLES; ENCODES INPUTS AND THEN DECODES THAT THROUGHPUT; FEATURE EXTRACTION ENGINE, SINCE MOST IMPORTANT FEATURES FOR RECOGNITION ARE EXTRACTED; USUALLY VERY SHALLOW NETWORKS; IN RESTRICTED BOLTZMANN MACHINE THE INPUT LAYER AND OUTPUT LAYER ARE IDENTICAL; IN MOST AUTOENCODERS THIS NEED NOT NECESSARILY BE, BUT MOST OF THE TIME THIS IS SO; BACKPROPAGATION WITH LOSS IS USED INSTEAD OF A COST FUNCTION; DEEP AUTOENCODERS ALSO EXIST, CAN BE USED FOR DIMENSIONALITY REDUCTION; PERFORM BETTER THAN PRINCIPAL COMPONENT ANALYSIS

RECURSIVE NEURAL TENSOR NETWORKS: BEST FOR HIERARCHICAL STRUCTURE RECOGNITION; MULTIPLE LEAF-GROUPS PROCESS THE INPUT AND GIVE THEIR OUTPUT TO A SINGLE ROOT GROUP; ROOT OUTPUTS CLASS (CLASS) AND SCORE (QUALITY OF PREDICTION); IN THE NEXT LAYER ONE ROOT IS TAKEN AS A LEAF AND ALL THE LEAVES ARE THEN USED AS INPUT FOR A NEW ROOT, ETC.; THE SCORE-VALUE IS USED TO DETERMINE THE BEST STRUCTURE TO USE; SUPERVISED AND USE BACKPROPAGATION; USED FOR NATURAL LANGUAGE PROCESSING AND IMAGE ANALYSIS WHEN IMAGE CONTAINS MANY COMPONENTS
SOCHER

USED FOR: MACHINE VISION; IMAGE CLASSIFICATION; VIDEO RECOGNITION; SPEECH RECOGNITION; TEXT PROCESSING; MACHINE TRANSLATION; SENTIMENT ANALYSIS; DOCUMENT CLASSIFICATION; MEDICAL APPLICATIONS SUCH AS DETECTION OF SYMPTOMS OR GENETIC POOLING; DRUG DISCOVERY; FINANCE; FRAUD/ANOMALY DETECTION; AGRICULTURE
