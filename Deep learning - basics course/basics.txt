NODES AND EDGES JOINED TOGETHER IN A GRAPH-LIKE STRUCTURE, AN ORDERED NETWORK
THE ORDERING IS: INPUT LAYER - HIDDEN LAYER(S) - OUPTUT LAYER
NEURAL NETWORKS CAN BE USED QUITE WELL FOR PATTERN RECOGNITION AND CLASSIFICATION
CLASSIFIERS 'FIRE' A CONFIDENCE SCORE BASED ON THE INPUT; DEEP NEURAL NETWORKS ARE ONLY CONSIDERED AS CLASSIFIERS IN THIS COURSE
THE 'FIRING' FROM THE INPUT LAYER TO THE OUTPUT LAYER IS FORWARD PROPAGATION; THE CLASSIFICATION HAPPENS BASED ON THE OUTPUT IN THE OUTPUT LAYER
EACH EDGE HAS UNIQUE WEIGHT AND EACH NODE UNIQUE BIAS; TO ADAPT WEIGHTS AND BIASES, BACKWARD PROPAGATION IS USED
COST IS COMPARISON OF OUTPUT OF CLASSIFICATION AND THE REAL CLASS

NEURAL NETWORKS MADE TO IMPROVE ACCURACY OF A SINGLE PERCEPTRON, THUS CREATING A LAYERED WEB OF PERCEPTRONS (MULTI-LAYERED PERCEPTRON, MLP)

PATTERN RECOGNITION HAS BEEN A DIFFICULT FIELD FOR COMPUTER SCIENCE, BUT IS BECOMING BETTER THANKS TO DEEP LEARNING
- PATTERN COMPLEXITY IS A REASON TO SWITCH TO NEURAL NETWORKS, SINCE THEY OUTPERFORM COMPETITION
- THIS IS ACHIEVED BY BREAKING FEATURES INTO SIMPLER FEATURES (SPECULATIVE!)
- GPU'S CAN PERFORM THE NEEDED CALCULATION QUITE FAST, MAKING IT MORE ATTRACTIVE TO SWITCH TO NEURAL NETWORKS

UNSUPERVISED/PATTERN RECOGNITION: RESTRICTED BOLTZMANN MACHINE, AUTOENCODER
SUPERVISED/CLASSIFICATION: TEXT PROCESSING (RECURRENT NETWORKS, RECURSIVE NEURAL TENSOR NETWORK (RNTN)), IMAGE RECOGNITION (DEEP BELIEF NETWORK (DBN), CONVOLUTIONAL NETWORKS), OBJECT RECOGNITION (RECURSIVE NEURAL TENSOR NETWORK (RNTN), CONVOLUTIONAL NETWORKS), SPEECH RECOGNITION (RECURRENT NETWORKS), CLASSIC CLASSIFICATION (MULTILAYER PERCEPTRONS WITH RECTIFIED LINEAR UNITS (MLP/RELU)), TIME SERIES ANALYSIS (RECURRENT NETWORK)

PROBLEM WITH TRAINING USING BACKPROPAGATION: VANISHING OR EXPLODING GRADIENT, THIS SINCE WITH PRODUCT RULE, PRODUCTS VANISH IF SOME PARTS ARE VANISHING, WHICH AFFECTS THE INPUT/LOW LAYERS MOST AND RESULTS IN A LOT OF TRAINING TIME FOR THOSE SHALLOW LAYERS
