DEEP LEARNING MORE POPULAR DUE TO INCREASED COMPUTING POWER, INCREASED MEMORY STORAGE AND ADVANCING OF ALGORITHMS
CLASSICALLY FEATURE EXTRACTION AND FEATURE SELECTION USED FOR IMAGE RECOGNITION, NOW THIS IS DONE AUTOMATICALLY
CONVOLUTIONAL NEURAL NETWORKS USED FOR IMAGE RECOGNITION AND, THEREFORE, ALSO MEDICAL DIAGNOSES
PIPELINE: PRE-PROCESSING -> TRAINING -> INFERENCE
SLOW: TRAINING IS SLOW, BUILDING NETWORK IS SLOW, UPDATING TRAINED MODEL IS SLOW (WITH NEW DATA OR NEW FEATURES)
FOR SPEEDING UP TRAINING, TENSOR PROCESSING UNIT (TPU) OR GRAPHICAL PROCESSING UNIT (GPU) SHOULD BE USED

OFTEN FORWARD PASS AND BACKWARD PASS NEED TO BE REPEATED MANY TIMES FOR TRAINING
HIGH-DIMENSIONAL DATA WHICH NEED TO BE MULTIPLIED MANY TIMES
MATRIX MULTIPLICATIONS ARE AUTMOATICALLY PARALLELLIZED USING GPU'S

CENTRAL PROCESSING UNIT (CPU): (MICOR-)PROCESSOR OF MOST COMPUTERS; EXECUTES INSTRUCTIONS SEQUENTIALLY BY READING DATA OUT LINE PER LINE; VERY FAST AT FETCHING AND PROCESSING SMALLER PIECES OF DATA
GRAPHICS PROCESSING UNIT (GPU): CHIP/PROCESSOR FOR RENDERING IMAGES; HAS MANY CORES AND CONCURRENT THREADS OPERATING IN PARALLEL; GPU ARE GOOD AT STORING LARGER AMOUNT OF DATA, NOT NECESSARILY VERY FAST AT FETCHING IT

NVIDIA IS MAIN VENDOR OF GPU'S AND CUDA RUNS ON TOP OF THAT, HIGH-LEVEL LANGUAGE
    MEMORY BANDWIDTH IMPORTANT METRIC FOR GPU
AMD IS ANOTHER VENDOR, BUT OPENCL NOT A VERY EASY AND POPULAR LANGUAGE
TENSORFLOW PROCESSING UNITS (TPU), GOOGLE HARDWARE FOR TENSORFLOW
FIELD PROGRAMMABLE GATE ARRAYS ARE CUSTOMIZABLE

DISADVANTAGES OF GPU: LIMITED MEMORY STORAGE AND NOT EASY INTEGRATABLE INTO EXISTING SYSTEMS
SOME COMPUTERS HAVE EMBEDDED (NVIDIA) GPU'S, BUT FAST IS TO USE GPU ON EXTERNAL COMPUTER SUCH AS WITH IBM CLOUD, AMAZON WEB SERVICES AND GOOGLE CLOUD

MOST POPULAR DEEP LEARNING FRAMEWORKS SCALE TO MULTIPLE GPU'S WITHIN ONE SERVER, BUT NOT TO MULTIPLE SERVERS WITH MULTIPLE GPU'S
DISTRIBUTED DEEP LEARNING AIMS TO ADDRESS THIS, ENABLING PARALLELLIZATION ACROSS HUNDREDS OF GPU'S ACROSS DOZENS OF SERVERS
